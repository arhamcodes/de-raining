{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh9po780BCxz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4YsDIGNAJyq"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/areeb-given-sorted-v1.zip -d /content/rain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1ULeQgjABEi"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "\n",
        "def set_autosave_interval(interval):\n",
        "    display(Javascript(f'google.colab._runtime.queueing.setAutosaveInterval({interval})'))\n",
        "\n",
        "# Set autosave interval to 0 seconds to minimize autosave frequency\n",
        "set_autosave_interval(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs6q08zjKaWQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset_name=\"/content/rain\", img_res=(256,256)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "        self.n_batches = 0\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.img_res),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def load_data(self, batch_size=1, is_testing=True):\n",
        "        from glob import glob\n",
        "        import numpy as np\n",
        "        # data_type = \"training\" if not is_testing else \"test_nature\"\n",
        "        data_type = \"training\" if not is_testing else \"test\"\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "        imgs_A, imgs_B = [], []\n",
        "\n",
        "        for img_path in batch_images:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            w, h = img.size\n",
        "            half_w = w // 2\n",
        "            img_A = img.crop((0, 0, half_w, h))\n",
        "            img_B = img.crop((half_w, 0, w, h))\n",
        "\n",
        "            if not is_testing and random.random() < 0.5:\n",
        "                img_A = transforms.functional.hflip(img_A)\n",
        "                img_B = transforms.functional.hflip(img_B)\n",
        "\n",
        "            img_A = self.transform(img_A)  # shape [3, H, W]\n",
        "            img_B = self.transform(img_B)\n",
        "            imgs_A.append(img_A)\n",
        "            imgs_B.append(img_B)\n",
        "\n",
        "        imgs_A = torch.stack(imgs_A, dim=0)  # shape [batch, 3, H, W]\n",
        "        imgs_B = torch.stack(imgs_B, dim=0)\n",
        "        return imgs_A, imgs_B\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=True):\n",
        "        from glob import glob\n",
        "        import numpy as np\n",
        "        # data_type = \"training\" if not is_testing else \"test_syn\"\n",
        "        # data_type = \"training\" if not is_testing else \"test_nature\"\n",
        "        data_type = \"training\" if not is_testing else \"test\"\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "        self.n_batches = int(len(path) / batch_size)\n",
        "\n",
        "        for i in range(self.n_batches - 1):\n",
        "            batch = path[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img_path in batch:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                w, h = img.size\n",
        "                half_w = w // 2\n",
        "                img_A = img.crop((0, 0, half_w, h))\n",
        "                img_B = img.crop((half_w, 0, w, h))\n",
        "\n",
        "                if not is_testing and random.random() > 0.5:\n",
        "                    img_A = transforms.functional.hflip(img_A)\n",
        "                    img_B = transforms.functional.hflip(img_B)\n",
        "\n",
        "                img_A = self.transform(img_A)\n",
        "                img_B = self.transform(img_B)\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = torch.stack(imgs_A, dim=0)\n",
        "            imgs_B = torch.stack(imgs_B, dim=0)\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "    def imread(self, path):\n",
        "        # Not really used in this PyTorch version, but kept for reference\n",
        "        return Image.open(path).convert('RGB')\n",
        "# ...existing code..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNn8aJEYgOsN"
      },
      "outputs": [],
      "source": [
        "# ...existing code...\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "# from data_loader import DataLoader\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=6):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.down2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 256, kernel_size=3, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.down3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.down4 = nn.Sequential(\n",
        "            nn.Conv2d(512, 64, kernel_size=3, stride=1),\n",
        "            nn.PReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Simple pyramid pool replacement\n",
        "        self.conv_out = nn.Sequential(\n",
        "            nn.Conv2d(64, 72, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, imgA, imgB):\n",
        "        imgA = F.interpolate(imgA, size=(256, 256))\n",
        "        imgB = F.interpolate(imgB, size=(256, 256))\n",
        "        x = torch.cat((imgA, imgB), dim=1)\n",
        "        x = self.down1(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.down3(x)\n",
        "        x = self.down4(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "# class DenseBlock(nn.Module):\n",
        "#     def __init__(self, num_layers, growth):\n",
        "#         super(DenseBlock, self).__init__()\n",
        "#         self.layers = nn.ModuleList()\n",
        "#         for _ in range(num_layers):\n",
        "#             self.layers.append(nn.Sequential(\n",
        "#                 nn.Conv2d(growth, growth, kernel_size=3, padding=1),\n",
        "#                 nn.BatchNorm2d(growth),\n",
        "#                 nn.LeakyReLU(0.2, inplace=True)\n",
        "#             ))\n",
        "#         self.growth = growth\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         features = [x]\n",
        "#         for layer in self.layers:\n",
        "#             out = layer(x)\n",
        "#             x = torch.cat([x, out], dim=1)\n",
        "#         return x\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_layers, growth):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.growth = growth\n",
        "        for i in range(num_layers):\n",
        "            self.layers.append(nn.Sequential(\n",
        "                nn.Conv2d(growth * (i + 1), growth, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(growth),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            out = layer(torch.cat(features, dim=1))\n",
        "            features.append(out)\n",
        "        return torch.cat(features, dim=1)\n",
        "\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.down1 = nn.Sequential(\n",
        "#             nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(2)\n",
        "#         )\n",
        "#         self.dense1 = DenseBlock(num_layers=4, growth=64)\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv2d(64+256, 128, kernel_size=3, stride=2, padding=1),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dense2 = DenseBlock(num_layers=6, growth=128)\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.Conv2d(128+768, 256, kernel_size=3, stride=2),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dense3 = DenseBlock(num_layers=8, growth=256)\n",
        "#         self.conv3 = nn.Sequential(\n",
        "#             nn.Conv2d(256+2048, 512, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dense4 = DenseBlock(num_layers=8, growth=512)\n",
        "#         self.conv4 = nn.Sequential(\n",
        "#             nn.Conv2d(512+4096, 128, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.up1 = nn.Sequential(\n",
        "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "#             nn.Conv2d(128, 120, kernel_size=4, stride=1, padding=1),\n",
        "#             nn.Dropout(0.0),\n",
        "#             nn.BatchNorm2d(120),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dense5 = DenseBlock(num_layers=6, growth=120)\n",
        "#         self.up2 = nn.Sequential(\n",
        "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "#             nn.Conv2d(120+720, 64, kernel_size=4, stride=1, padding=1),\n",
        "#             nn.Dropout(0.0),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dense6 = DenseBlock(num_layers=4, growth=64)\n",
        "#         self.up3 = nn.Sequential(\n",
        "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "#             nn.Conv2d(64+256, 64, kernel_size=4, stride=1, padding=1),\n",
        "#             nn.Dropout(0.0),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.dense7 = DenseBlock(num_layers=4, growth=64)\n",
        "#         self.conv5 = nn.Sequential(\n",
        "#             nn.Conv2d(64+256, 16, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "#         self.pad = nn.ReflectionPad2d(5)\n",
        "#         self.conv6 = nn.Conv2d(16, 3, kernel_size=3)\n",
        "#         self.out = nn.Tanh()\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.down1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.dense1 = DenseBlock(num_layers=4, growth=64)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(64+256, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dense2 = DenseBlock(num_layers=6, growth=128)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128+768, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dense3 = DenseBlock(num_layers=8, growth=256)\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256+2048, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dense4 = DenseBlock(num_layers=8, growth=512)\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(512+4096, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(128, 120, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.BatchNorm2d(120),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dense5 = DenseBlock(num_layers=6, growth=120)\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(120+720, 64, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dense6 = DenseBlock(num_layers=4, growth=64)\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64+256, 64, kernel_size=4, stride=1, padding=1),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dense7 = DenseBlock(num_layers=4, growth=64)\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(64+256, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.pad = nn.ReflectionPad2d(4)  # Adjust padding to ensure output size matches input size\n",
        "        self.conv6 = nn.Conv2d(16, 3, kernel_size=3, padding=1)  # Adjust padding to ensure output size matches input size\n",
        "        self.out = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.down1(x)\n",
        "        db1 = self.dense1(x0)\n",
        "        c1 = self.conv1(db1)\n",
        "        db2 = self.dense2(c1)\n",
        "        c2 = self.conv2(db2)\n",
        "        db3 = self.dense3(c2)\n",
        "        c3 = self.conv3(db3)\n",
        "        db4 = self.dense4(c3)\n",
        "        c4 = self.conv4(db4)\n",
        "        u1 = self.up1(c4)\n",
        "        db5 = self.dense5(u1)\n",
        "        u2 = self.up2(db5)\n",
        "        db6 = self.dense6(u2)\n",
        "        u3 = self.up3(db6)\n",
        "        db7 = self.dense7(u3)\n",
        "        c5 = self.conv5(db7)\n",
        "        c5 = self.pad(c5)\n",
        "        c6 = self.conv6(c5)\n",
        "        return self.out(c6)\n",
        "\n",
        "# class IDGAN:\n",
        "#     def __init__(self):\n",
        "#         self.img_rows = 256\n",
        "#         self.img_cols = 256\n",
        "#         self.channels = 3\n",
        "#         self.img_shape = (self.channels, self.img_rows, self.img_cols)\n",
        "#         self.dataset_name = 'rain'\n",
        "#         self.data_loader = DataLoader(dataset_name=self.dataset_name, img_res=(self.img_rows, self.img_cols))\n",
        "#         self.disc_out = (72, 14, 14)\n",
        "#         self.generator = Generator().cuda()\n",
        "#         self.discriminator = Discriminator().cuda()\n",
        "#         self.optim_g = optim.Adam(self.generator.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
        "#         self.optim_d = optim.SGD(self.discriminator.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-6, nesterov=False)\n",
        "#         self.adversarial_criterion = nn.MSELoss()\n",
        "#         self.l1_criterion = nn.L1Loss()\n",
        "\n",
        "#     def train(self, epochs, batch_size, sample_interval=28):\n",
        "#         valid = torch.ones(batch_size, *self.disc_out).cuda()\n",
        "#         fake = torch.zeros(batch_size, *self.disc_out).cuda()\n",
        "#         start_time = datetime.datetime.now()\n",
        "\n",
        "#         for epoch in range(epochs):\n",
        "#             for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "#                 # print(imgs_A.shape, imgs_B.shape)\n",
        "#                 real_A = torch.FloatTensor(imgs_A).cuda()\n",
        "#                 real_B = torch.FloatTensor(imgs_B).cuda()\n",
        "\n",
        "#                 # Train Discriminator\n",
        "#                 self.discriminator.train()\n",
        "#                 self.optim_d.zero_grad()\n",
        "#                 fake_A = self.generator(real_B)\n",
        "#                 fake_A = F.interpolate(fake_A, size=(256, 256))\n",
        "#                 pred_real = self.discriminator(real_A, real_B)\n",
        "#                 loss_real = self.adversarial_criterion(pred_real, valid)\n",
        "#                 pred_fake = self.discriminator(fake_A.detach(), real_B)\n",
        "#                 loss_fake = self.adversarial_criterion(pred_fake, fake)\n",
        "#                 d_loss = 0.5 * (loss_real + loss_fake)\n",
        "#                 d_loss.backward()\n",
        "#                 self.optim_d.step()\n",
        "\n",
        "#                 # Train Generator\n",
        "#                 self.generator.train()\n",
        "#                 self.optim_g.zero_grad()\n",
        "#                 pred_fake = self.discriminator(fake_A, real_B)\n",
        "#                 g_loss_adv = self.adversarial_criterion(pred_fake, valid)\n",
        "#                 l1_loss = self.l1_criterion(fake_A, real_A)\n",
        "#                 g_loss = g_loss_adv + 10.0 * l1_loss\n",
        "#                 g_loss.backward()\n",
        "#                 self.optim_g.step()\n",
        "\n",
        "#                 elapsed_time = datetime.datetime.now() - start_time\n",
        "#                 print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\"\n",
        "#                       % (epoch, epochs, batch_i, self.data_loader.n_batches, d_loss.item(), g_loss.item(), elapsed_time))\n",
        "\n",
        "#                 if batch_i % sample_interval == 0:\n",
        "#                     self.sample_images(epoch, batch_i, real_A, fake_A)\n",
        "\n",
        "#     # def sample_images(self, epoch, batch_i, real_A, fake_A):\n",
        "#     #     os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "#     #     r, c = 3, 3\n",
        "#     #     real_A_np = real_A.permute(0,2,3,1).cpu().data.numpy()\n",
        "#     #     fake_A_np = fake_A.permute(0,2,3,1).cpu().data.numpy()\n",
        "#     #     fig, axs = plt.subplots(r, c)\n",
        "#     #     gen_imgs = np.concatenate([fake_A_np, real_A_np, real_A_np])\n",
        "#     #     gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "#     #     titles = ['WithRain', 'Generated', 'Original']\n",
        "#     #     cnt = 0\n",
        "#     #     for i in range(r):\n",
        "#     #         for j in range(c):\n",
        "#     #             axs[i,j].imshow(gen_imgs[cnt])\n",
        "#     #             axs[i,j].set_title(titles[i])\n",
        "#     #             axs[i,j].axis('off')\n",
        "#     #             cnt += 1\n",
        "#     #     fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "#     #     plt.close()\n",
        "\n",
        "#     def sample_images(self, epoch, batch_i, real_A, fake_A):\n",
        "#         os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "#         r, c = 1, 3  # Adjust grid size to 1x3\n",
        "#         real_A_np = real_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "#         fake_A_np = fake_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "#         fig, axs = plt.subplots(r, c)\n",
        "#         gen_imgs = np.concatenate([real_A_np, fake_A_np, real_A_np])\n",
        "#         gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "#         titles = ['Original', 'Generated', 'Original']\n",
        "#         cnt = 0\n",
        "#         for i in range(r):\n",
        "#             for j in range(c):\n",
        "#                 axs[j].imshow(gen_imgs[cnt])  # Adjust indexing to axs[j]\n",
        "#                 axs[j].set_title(titles[j])  # Adjust indexing to titles[j]\n",
        "#                 axs[j].axis('off')\n",
        "#                 cnt += 1\n",
        "#         fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "#         plt.close()\n",
        "\n",
        "# gan = IDGAN()\n",
        "# gan.train(epochs=150, batch_size=8, sample_interval=25)\n",
        "\n",
        "#######################################################################################\n",
        "\n",
        "# class IDGAN:\n",
        "#     def __init__(self):\n",
        "#         self.img_rows = 256\n",
        "#         self.img_cols = 256\n",
        "#         self.channels = 3\n",
        "#         self.img_shape = (self.channels, self.img_rows, self.img_cols)\n",
        "#         self.dataset_name = 'rain'\n",
        "#         self.data_loader = DataLoader(dataset_name=self.dataset_name, img_res=(self.img_rows, self.img_cols))\n",
        "#         self.disc_out = (72, 14, 14)\n",
        "#         self.generator = Generator().cuda()\n",
        "#         self.discriminator = Discriminator().cuda()\n",
        "#         self.optim_g = optim.Adam(self.generator.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
        "#         self.optim_d = optim.SGD(self.discriminator.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-6, nesterov=False)\n",
        "#         self.adversarial_criterion = nn.MSELoss()\n",
        "#         self.l1_criterion = nn.L1Loss()\n",
        "#         self.checkpoint_dir = '/content/drive/MyDrive/checkpoints'  # Update to Google Drive directory\n",
        "#         os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "#     def save_checkpoint(self, epoch):\n",
        "#         torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'generator_state_dict': self.generator.state_dict(),\n",
        "#             'discriminator_state_dict': self.discriminator.state_dict(),\n",
        "#             'optimizer_g_state_dict': self.optim_g.state_dict(),\n",
        "#             'optimizer_d_state_dict': self.optim_d.state_dict(),\n",
        "#         }, os.path.join(self.checkpoint_dir, f'checkpoint_{epoch}.pth'))\n",
        "\n",
        "#     def train(self, epochs, batch_size, sample_interval=28, checkpoint_interval=10):\n",
        "#         valid = torch.ones(batch_size, *self.disc_out).cuda()\n",
        "#         fake = torch.zeros(batch_size, *self.disc_out).cuda()\n",
        "#         start_time = datetime.datetime.now()\n",
        "\n",
        "#         for epoch in range(epochs):\n",
        "#             for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "#                 real_A = torch.FloatTensor(imgs_A).cuda()\n",
        "#                 real_B = torch.FloatTensor(imgs_B).cuda()\n",
        "\n",
        "#                 # Train Discriminator\n",
        "#                 self.discriminator.train()\n",
        "#                 self.optim_d.zero_grad()\n",
        "#                 fake_A = self.generator(real_B)\n",
        "#                 fake_A = F.interpolate(fake_A, size=(256, 256))\n",
        "#                 pred_real = self.discriminator(real_A, real_B)\n",
        "#                 loss_real = self.adversarial_criterion(pred_real, valid)\n",
        "#                 pred_fake = self.discriminator(fake_A.detach(), real_B)\n",
        "#                 loss_fake = self.adversarial_criterion(pred_fake, fake)\n",
        "#                 d_loss = 0.5 * (loss_real + loss_fake)\n",
        "#                 d_loss.backward()\n",
        "#                 self.optim_d.step()\n",
        "\n",
        "#                 # Train Generator\n",
        "#                 self.generator.train()\n",
        "#                 self.optim_g.zero_grad()\n",
        "#                 pred_fake = self.discriminator(fake_A, real_B)\n",
        "#                 g_loss_adv = self.adversarial_criterion(pred_fake, valid)\n",
        "#                 l1_loss = self.l1_criterion(fake_A, real_A)\n",
        "#                 g_loss = g_loss_adv + 10.0 * l1_loss\n",
        "#                 g_loss.backward()\n",
        "#                 self.optim_g.step()\n",
        "\n",
        "#                 elapsed_time = datetime.datetime.now() - start_time\n",
        "#                 print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\"\n",
        "#                       % (epoch, epochs, batch_i, self.data_loader.n_batches, d_loss.item(), g_loss.item(), elapsed_time))\n",
        "\n",
        "#                 if batch_i % sample_interval == 0:\n",
        "#                     self.sample_images(epoch, batch_i, real_A, fake_A)\n",
        "\n",
        "#             # Save checkpoint\n",
        "#             if epoch % checkpoint_interval == 0:\n",
        "#                 self.save_checkpoint(epoch)\n",
        "\n",
        "#     def sample_images(self, epoch, batch_i, real_A, fake_A):\n",
        "#         os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "#         r, c = 1, 3  # Adjust grid size to 1x3\n",
        "#         real_A_np = real_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "#         fake_A_np = fake_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "#         fig, axs = plt.subplots(r, c)\n",
        "#         gen_imgs = np.concatenate([real_A_np, fake_A_np, real_A_np])\n",
        "#         gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "#         titles = ['Original', 'Generated', 'Original']\n",
        "#         cnt = 0\n",
        "#         for i in range(r):\n",
        "#             for j in range(c):\n",
        "#                 axs[j].imshow(gen_imgs[cnt])  # Adjust indexing to axs[j]\n",
        "#                 axs[j].set_title(titles[j])  # Adjust indexing to titles[j]\n",
        "#                 axs[j].axis('off')\n",
        "#                 cnt += 1\n",
        "#         fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "#         plt.close()\n",
        "\n",
        "# gan = IDGAN()\n",
        "# gan.train(epochs=150, batch_size=8, sample_interval=25, checkpoint_interval=10)\n",
        "\n",
        "# # ...existing code...\n",
        "\n",
        "######################################################################################################\n",
        "\n",
        "class IDGAN:\n",
        "    def __init__(self):\n",
        "        self.img_rows = 256\n",
        "        self.img_cols = 256\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.channels, self.img_rows, self.img_cols)\n",
        "        self.dataset_name = 'rain'\n",
        "        self.data_loader = DataLoader(dataset_name=self.dataset_name, img_res=(self.img_rows, self.img_cols))\n",
        "        self.disc_out = (72, 14, 14)\n",
        "        self.generator = Generator().cuda()\n",
        "        self.discriminator = Discriminator().cuda()\n",
        "        self.optim_g = optim.Adam(self.generator.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "        self.optim_d = optim.SGD(self.discriminator.parameters(), lr=2e-3, momentum=0.9, weight_decay=1e-6, nesterov=False)\n",
        "        self.adversarial_criterion = nn.MSELoss()\n",
        "        self.l1_criterion = nn.L1Loss()\n",
        "        self.checkpoint_dir = '/content/drive/MyDrive/checkpoints'  # Update to Google Drive directory\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    def save_checkpoint(self, epoch):\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': self.generator.state_dict(),\n",
        "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
        "            'optimizer_g_state_dict': self.optim_g.state_dict(),\n",
        "            'optimizer_d_state_dict': self.optim_d.state_dict(),\n",
        "        }, os.path.join(self.checkpoint_dir, f'checkpoint_{epoch}.pth'))\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "        self.optim_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
        "        self.optim_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
        "        return checkpoint['epoch']\n",
        "\n",
        "    def train(self, epochs, batch_size, sample_interval=28, checkpoint_interval=20, resume_from_checkpoint=None):\n",
        "        start_epoch = 0\n",
        "        if resume_from_checkpoint:\n",
        "            start_epoch = self.load_checkpoint(resume_from_checkpoint)\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "        valid = torch.ones(batch_size, *self.disc_out).cuda()\n",
        "        fake = torch.zeros(batch_size, *self.disc_out).cuda()\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "                real_A = torch.FloatTensor(imgs_A).cuda()\n",
        "                real_B = torch.FloatTensor(imgs_B).cuda()\n",
        "\n",
        "                # Train Discriminator\n",
        "                self.discriminator.train()\n",
        "                self.optim_d.zero_grad()\n",
        "                fake_A = self.generator(real_B)\n",
        "                fake_A = F.interpolate(fake_A, size=(256, 256))\n",
        "                pred_real = self.discriminator(real_A, real_B)\n",
        "                loss_real = self.adversarial_criterion(pred_real, valid)\n",
        "                pred_fake = self.discriminator(fake_A.detach(), real_B)\n",
        "                loss_fake = self.adversarial_criterion(pred_fake, fake)\n",
        "                d_loss = 0.5 * (loss_real + loss_fake)\n",
        "                d_loss.backward()\n",
        "                self.optim_d.step()\n",
        "\n",
        "                # Train Generator\n",
        "                self.generator.train()\n",
        "                self.optim_g.zero_grad()\n",
        "                pred_fake = self.discriminator(fake_A, real_B)\n",
        "                g_loss_adv = self.adversarial_criterion(pred_fake, valid)\n",
        "                l1_loss = self.l1_criterion(fake_A, real_A)\n",
        "                g_loss = g_loss_adv + 10.0 * l1_loss\n",
        "                g_loss.backward()\n",
        "                self.optim_g.step()\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "                print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\"\n",
        "                      % (epoch, epochs, batch_i, self.data_loader.n_batches, d_loss.item(), g_loss.item(), elapsed_time))\n",
        "\n",
        "                if batch_i % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_i, real_A, fake_A)\n",
        "\n",
        "            # Save checkpoint\n",
        "            if epoch % checkpoint_interval == 0:\n",
        "                self.save_checkpoint(epoch)\n",
        "\n",
        "    def test(self, checkpoint_path, test_data_loader, output_dir='test_images'):\n",
        "      # Load the checkpoint\n",
        "      self.load_checkpoint(checkpoint_path)\n",
        "\n",
        "      # Create output directory if it doesn't exist\n",
        "      os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "      # Set the generator to evaluation mode\n",
        "      self.generator.eval()\n",
        "\n",
        "      for i, (imgs_A, imgs_B) in enumerate(test_data_loader.load_batch(1)):\n",
        "          real_A = torch.FloatTensor(imgs_A).cuda()\n",
        "          real_B = torch.FloatTensor(imgs_B).cuda()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              fake_A = self.generator(real_B)\n",
        "              fake_A = torch.nn.functional.interpolate(fake_A, size=(256, 256))\n",
        "\n",
        "          # self.save_test_images(real_A, fake_A, i, output_dir)\n",
        "\n",
        "    def save_test_images(self, real_A, fake_A, image_index, output_dir):\n",
        "        real_A_np = real_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "        fake_A_np = fake_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "\n",
        "        gen_imgs = np.concatenate([real_A_np, fake_A_np, real_A_np])\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Generated', 'Original']\n",
        "        fig, axs = plt.subplots(1, 3)\n",
        "\n",
        "        for j in range(3):\n",
        "            axs[j].imshow(gen_imgs[j])\n",
        "            axs[j].set_title(titles[j])\n",
        "            axs[j].axis('off')\n",
        "\n",
        "        fig.savefig(os.path.join(output_dir, f'{image_index}.png'))\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    def sample_images(self, epoch, batch_i, real_A, fake_A):\n",
        "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "        r, c = 1, 3  # Adjust grid size to 1x3\n",
        "        real_A_np = real_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "        fake_A_np = fake_A.permute(0, 2, 3, 1).cpu().data.numpy()\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        gen_imgs = np.concatenate([real_A_np, fake_A_np, real_A_np])\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "        titles = ['Original', 'Generated', 'Original']\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[j].imshow(gen_imgs[cnt])  # Adjust indexing to axs[j]\n",
        "                axs[j].set_title(titles[j])  # Adjust indexing to titles[j]\n",
        "                axs[j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "gan = IDGAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAwfKvcHfgAn"
      },
      "outputs": [],
      "source": [
        "# To resume training from a checkpoint, provide the path to the checkpoint file\n",
        "gan.train(epochs=12500, batch_size=8, sample_interval=25, checkpoint_interval=10, resume_from_checkpoint='/content/drive/MyDrive/checkpoints/checkpoint_420.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EsunvlMjh9qK"
      },
      "outputs": [],
      "source": [
        "test_data_loader = DataLoader(dataset_name='rain', img_res=(256, 256))\n",
        "gan.test(checkpoint_path='/content/drive/MyDrive/checkpoints/checkpoint_420.pth', test_data_loader=test_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLErA5qMhtWS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
